import streamlit as st
import pandas as pd
import requests
import io
import re

# 1. Page Configuration & RTL (Right-to-Left) Fix for Hebrew
st.set_page_config(page_title=" 专注  专爪", layout="centered")
st.markdown("""
    <style>
        .stApp { direction: rtl; text-align: right; }
        .stSelectbox label { text-align: right; width: 100%; }
        /* Make the table look good on mobile */
        [data-testid="stDataFrame"] { width: 100%; } 
    </style>
""", unsafe_allow_html=True)

st.title("  专注 -  专爪")
st.write("专 拽爪  专转 转  专注 专 注专.")

# 2. Download and Cache Data (Downloads once every 5 minutes to keep it fast)
@st.cache_data(ttl=300)
def load_data():
    sheet_id = "1YlWC_x_ZZtR22p-R1bI_Nvl015bbE7ge"
    url = f"https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=xlsx&gid=1957947665"
    
    response = requests.get(url)
    response.raise_for_status()
    raw_df = pd.read_excel(io.BytesIO(response.content), header=None)

    # Find headers
    header_idx = -1
    for idx, row in raw_df.iterrows():
        if any("拽爪转 砖转转驻转" in str(cell) for cell in row.values):
            header_idx = idx
            break

    if header_idx == -1:
        st.error("砖:  爪 注转 '拽爪转 砖转转驻转'.")
        return None, None

    # Set headers and cleanup
    raw_df.columns = [str(c).strip() for c in raw_df.iloc[header_idx]]
    df = raw_df.iloc[header_idx + 1:].reset_index(drop=True)
    
    # Identify specific columns
    col_mapping = {
        'target': next((c for c in df.columns if "拽爪转 砖转转驻转" in c), None),
        'week': next((c for c in df.columns if "砖注" in c or "转专" in c), None),
        'event': next((c for c in df.columns if "专注" in c), None),
        'loc': next((c for c in df.columns if "" in c or "拽" in c), None)
    }

    # Forward fill dates
    if col_mapping['week']:
        df[col_mapping['week']] = df[col_mapping['week']].ffill()

    return df, col_mapping

# 3. Main App Logic
df, col_mapping = load_data()

if df is not None and col_mapping['target']:
    # Extract unique groups
    unique_groups = set()
    for val in df[col_mapping['target']].dropna():
        parts = re.split(r'[,\n/]', str(val))
        for part in parts:
            clean_part = part.strip()
            if clean_part and clean_part.lower() != 'nan':
                unique_groups.add(clean_part)

    clean_groups = sorted(list(unique_groups))

    # Dropdown Menu
    selected_group = st.selectbox("驻砖 拽爪:", [""] + clean_groups)

    if selected_group:
        # Filter the data
        mask = df[col_mapping['target']].astype(str).str.contains(selected_group, regex=False, na=False)
        filtered_df = df[mask].copy()

        # Keep only the relevant columns in the exact order needed for Streamlit's visual layout
        display_cols = []
        rename_dict = {}
        
        # 1. Left-most column
        display_cols.append(col_mapping['target'])
        rename_dict[col_mapping['target']] = "拽爪转 砖转转驻转"

        # 2. Middle-left column
        if col_mapping['loc']: 
            display_cols.append(col_mapping['loc'])
            rename_dict[col_mapping['loc']] = "拽"
            
        # 3. Middle-right column
        if col_mapping['week']: 
            display_cols.append(col_mapping['week'])
            rename_dict[col_mapping['week']] = "砖注/转专"
            
        # 4. Right-most column
        if col_mapping['event']: 
            display_cols.append(col_mapping['event'])
            rename_dict[col_mapping['event']] = "专注"

        final_df = filtered_df[display_cols].rename(columns=rename_dict)
        
        # Clean up 'nan' text in the table
        final_df = final_df.fillna("").astype(str).replace('nan', '')

        st.success(f"爪 {len(final_df)} 专注 注专: {selected_group}")
        
        # Display the table
        st.dataframe(final_df, hide_index=True, use_container_width=True)